{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O pipeline é o fluxo pelo qual o dado passa para o seu processamento\n",
    "\n",
    "No pipeline, a coleta, transformação, carregamento/armazenamento e monitoramento de dados pode ocorre através de automação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual a diferença entre data wrangling e pipelines?\n",
    "\n",
    "- O data wrangling é algo mais simples e utilizada dados já prontos, há uma manipulação dos dados para torná-los 'usáveis'\n",
    "- O pipeline trabalha de maneira mais completa, podendo coletar/extrair de diferentes fontes para transformar os dados e torná-los 'usáveis'\n",
    "\n",
    "Em um exemplo:\n",
    "\n",
    "- Data wrangling: suponha que há um conjunto de dados com informações sobre clientes, mas as datas estão em um formato bagunçado. O data wrangling responsável pelo processo de organização desses dados para que possam ser facilmente compreendidos e utilizados, convertendo com uma espécie de padrão. A manipulação de dados é como organizar e preparar os dados. Isso envolve limpar os dados, moldá-los do jeito certo e garantir que tudo esteja adequado para a utilização.\n",
    "- Pipeline: em um processo de dados de de vendas em diferentes lojas, deseja-se analisar o desempenho total, o que envolve a extração de dados de cada loja, a transformação para garantir a consistência (a exemplo, unifcar formato de datas, moedas) e caregar os dados em único local para analisar. Nesse caso, o processo de mover os dados de um lugar para outro, transformá-los para atender as necessidades específicas e carregá-los em um local onde possam ser utilizados existe. Para esse processo dar-se o nome de ETL: E(Extract -> Coleta), T (Tranform -> Transformação) e L (Load -> Carregamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapas:\n",
    "\n",
    "- Coleta de dados: como os dados são coletados de diferentes fontes, é necessário checá-los antes. Normalmente em um projeto há as fontes de dados disponíveis, mas em outros ainda pode ser necessário uma busca mais aprofundada. \n",
    "    - Exploração de dados: verifica os tipos de dados das características, valores únicos e descrição dos dados.\n",
    "- Transformação: A limpeza e separação são trabalhados juntos, já que o processo de transformalçao dos dados ocorre de acordo com as necessidades do projeto, incluindo limpeza, normalização, agregação.\n",
    "    - Limpeza de dados: remoção de valores nulos, duplicatas ou outliers (dado errado que pode interferir nos resultados, como a altura de uma pessoa descrita como 300cm/300m) que possam afetar a qualidade dos dados\n",
    "    - Feature Engineering: novas caractrísticas são criadas para agregar nos dados finais (a exemplo, tenho a data de nascimento nos dados e quero converter para a idade)\n",
    "    - Transformação de dados: transforma os dados para melhor atender as preferências e/ou requisitos específicos (a exemplo converter os dados de uma moeda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de transformação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Criando um dataframe exemplo\n",
    "data = {'Produto': ['A', 'B', 'C', 'D', 'E'],\n",
    "        'Preço': [100.0, 120.0, None, 150.0, 5000.0]} #Incluindo um valor nulo (None) e outlier (5000)\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe original:\n",
      "  Produto   Preço\n",
      "0       A   100.0\n",
      "1       B   120.0\n",
      "2       C     NaN\n",
      "3       D   150.0\n",
      "4       E  5000.0\n"
     ]
    }
   ],
   "source": [
    "#Exibindo o dataframe original\n",
    "print('Dataframe original:')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpgn\\AppData\\Local\\Temp\\ipykernel_5640\\3577481104.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Preço'].fillna(df['Preço'].mean(), inplace=True) #-> Forma antiga\n"
     ]
    }
   ],
   "source": [
    "#Limpeza de Dados: tratando valores nulos e outliers na coluna 'Preço'\n",
    "#Substituindo valores nulos pela média e removendo outliers (valores acima de 1000)\n",
    "df['Preço'].fillna(df['Preço'].mean(), inplace=True) #-> Forma antiga\n",
    "#df.fillna({'Preço': 'Preço'}, inplace = True) #nova forma a partir do pandas 3.0 \n",
    "#df = df['Preço'].fillna(df['Preço'].mean()) #-> também pode ser usado no pandas 3.0\n",
    "df = df[df['Preço'] < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame após a limpeza:\n",
      "  Produto  Preço\n",
      "0       A  100.0\n",
      "1       B  120.0\n",
      "3       D  150.0\n"
     ]
    }
   ],
   "source": [
    "#Exibindo o dataframe após a limpeza\n",
    "print('\\nDataFrame após a limpeza:')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supondo que 'df' seja o DataFrame com os dados de vendas\n",
    "df['Data'] = pd.to_datetime(df['Data']) #Convertendo para o formato de data\n",
    "\n",
    "#Criando uma nova coluna com o dia da semana\n",
    "df['DiaDaSemana'] = df['Data'].dt.day_name()\n",
    "\n",
    "#One-Hot Encondig para a coluna 'DiaDaSemana' \n",
    "df = pdf.get_dummies(df, columns['DiaDaSemana'])\n",
    "\n",
    "#Exibindo as primeiras linhas do DataFrame após a transformação\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carregamento e armazenamento: armazenz os dados transformados em um local de destino, como banco de dados, um data warehouse ou mesmo um arquivo\n",
    "    - Carregamento de dados: coleta de todos dados de suas respectivas fontes\n",
    "    - Destino: depois de coletar os dados necessários (carregamento de dados), os dados são reunidos em um único arquivo e começam a ser preparados/moldados\n",
    "    - Análise/Relatórios: análise e/ou relatório ou outras ações que se deseja realizar nos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Suponhamos que há um arquivo CSV, banco de dados SQL e uma API\n",
    "df_csv = pd.read_csv('dados_csv.csv')\n",
    "\n",
    "#Conectando ao banco de dados SQL (Como o SQLite)\n",
    "engine = create_engine('sqlite:///banco_sql.bd')\n",
    "query_sql = '%SELECT * FROM tabela_sql'\n",
    "df_sql = pd.read_sql_query(query_sql, engine)\n",
    "\n",
    "#Supondo uma API fictícia (precisa da URL real da API e as credencias)\n",
    "api_url = 'https://api.exemplo.com/dados'\n",
    "df_api = pd.read_json(api_url)\n",
    "\n",
    "#Agora temos os dados de diferentes fontes. Vamos transformá-los\n",
    "\n",
    "#Transformação: a exemplo, adicionar uma nova coluna aos DataFrames\n",
    "df_csv['Nova_Coluna'] = df_csv['Coluna_A'] + df_csv['Coluna_B']\n",
    "df_sql['Nova_Coluna'] = df_sql['Coluna_C'] * 2\n",
    "df_api['Nova_Coluna'] = df_api['Coluna X'] - df_api['Coluna_Y']\n",
    "\n",
    "#Agora vamos concatenar (ou combinar) os DataFrames em um único DataFrame\n",
    "df_final = pd.concat([df_csv, df_sql, df_api], ignore_index = True)\n",
    "\n",
    "#Por fim, vamos carregar o DataFrame final em um arquivo CSV ou em um banco de dados\n",
    "\n",
    "#Salvando em um arquivo CSV\n",
    "df_final.to_csv('dados_final.csv', index=False)\n",
    "\n",
    "#Salvando no mesmo banco de ddos SQL\n",
    "df_final.to_sql('tabela_final', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monitoramento: monitorar o desempenho da pipeline, gerenciar falhas e lidar com situações excepcionais são partes críticas da administração de ume pipeline de dados\n",
    "    - Monitoramento de desempenho: monitorar o tempo que cada etapa leva para ser concluída\n",
    "    - Monitoramento de qualidade: verficar a qualidade dos dados em cada etapa\n",
    "    - Monitoramento de recursos: monitora o uso de recursos como capacidade de armazenamento e poder de processamento\n",
    "\n",
    "    - Diferentes tecnologias para monitoramento de dados:\n",
    "        - Apache Airflow\n",
    "        - Prometheus\n",
    "        - Grafana\n",
    "        - Datadog\n",
    "        - Azure Application Insights\n",
    "        - Python usando a bibloteca logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "#Configurando o logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "#Função de Extração\n",
    "def extract_data(file_path):\n",
    "    logging.info('Iniciando extração de dados...')\n",
    "    df = pd.read_csv(file_path)\n",
    "    loggind.info('Extração de dados concluída')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Automação: é a capacidade de realizar tarefas e processos de forma programada e sem intervenção manual. Isso é essencial para garantir eficiência, consistência e confiabilidade em todo o fluxo de dados. Aqui estão alguns aspectos-chhave da automação em uma pipeline de dados:\n",
    "    - Agendamento de tarefas\n",
    "    - Orquestração de fluxo de trabalho\n",
    "    - Gestão de dependências\n",
    "    - Monitoramento e notificação\n",
    "    - Tratamento de erros e retentativas\n",
    "    - Gerenciamento de configuração\n",
    "    - Atualizações automáticas\n",
    "    - Integração contínua e implantação contínua (CI/CD)\n",
    "    - Escalonamento automático"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
